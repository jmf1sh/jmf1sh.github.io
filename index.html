<!doctype html>

<html lang="en-us">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>jonathan&#39;s blog</title>
    <meta name="description" content="The HTML5 Herald">
    <meta name="author" content="Jonathan Fisher">
    <meta name="generator" content="Hugo 0.81.0" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css?family=Roboto+Slab|Ruda" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="https://jmf1sh.github.io/css/styles.css" />

    
   <script>
  MathJax = {
    loader: {load: ['[tex]/ams']},
    tex: {
      packages: {'[+]': ['ams']},
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
  });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </head>

  <body>
    <div id="container">
      <header>
        <h1>
          <a href="https://jmf1sh.github.io/">jonathan&rsquo;s blog</a>
        </h1>

        <ul id="social-media">
          

          
          <li><a href="https://www.linkedin.com/in/jonathanfisher10"><i class="fa fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
          

          
          <li><a href="https://github.com/jmf1sh"><i class="fa fa-github fa-lg" aria-hidden="true"></i></a></li>
          

          
          <li><a href="https://gitlab.com/jmf1sh"><i class="fa fa-gitlab fa-lg" aria-hidden="true"></i></a></li>
          

          

          
        </ul>
        
        <p><em>mathematics, physics, programming, and machine learning</em></p>
        
      </header>

      
<nav>
    <ul>
        
        <li>
            <a class="" href="https://jmf1sh.github.io/posts/">
                <i class="fa-li fa  fa-lg"></i><span>Posts</span>
            </a>
        </li>
        
        <li>
            <a class="" href="https://jmf1sh.github.io/about/">
                <i class="fa-li fa  fa-lg"></i><span>About</span>
            </a>
        </li>
        
    </ul>
</nav>

      <main>




<section id="home">
        <ul>
          
          <li class="first">
            <h1><a href="https://jmf1sh.github.io/posts/2018-12-24-vae/" title="Variational autoencoders">Variational autoencoders</a></h1> 
            <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2018-12-24T00:00:00Z">Dec 24, 2018</time>
        </li>
        

        

        <li>2 minutes read</li>
    </ul>
</aside>

            <p>See Autoencoding variational Bayes
Suppose we have set $X$ with (a priori unknown) probability density function $p(x)$. We would like to find a lower-dimensional representation of $X$ in some latent space $Z$ by imposing a joint probability density $p(x, z)$.</p>
            
            <a href="https://jmf1sh.github.io/posts/2018-12-24-vae/">Read more…</a>
            
          </li>
          
          <li >
            <h1><a href="https://jmf1sh.github.io/posts/2018-07-10-beta-function/" title="Euler beta function and Dirichlet distribution">Euler beta function and Dirichlet distribution</a></h1> 
            <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2018-07-10T00:00:00Z">Jul 10, 2018</time>
        </li>
        

        

        <li>2 minutes read</li>
    </ul>
</aside>

            <p>Let&rsquo;s remind ourselves how about the Euler beta function, defined as
$$ B(x,y) = \int_0^1 t^{x-1} (1-t)^{y-1} dt. $$
First, we will express $$B(x,y)$$ in terms of the more familiar Gamma function, defined as</p>
            
            <a href="https://jmf1sh.github.io/posts/2018-07-10-beta-function/">Read more…</a>
            
          </li>
          
          <li >
            <h1><a href="https://jmf1sh.github.io/posts/2018-01-14-reinforcement-learning/" title="Introduction to Reinforcement Learning">Introduction to Reinforcement Learning</a></h1> 
            <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2018-01-14T00:00:00Z">Jan 14, 2018</time>
        </li>
        

        

        <li>4 minutes read</li>
    </ul>
</aside>

            <p>I would like to record the basic formalism of reinforcement learning. Hopefully, this will lead to a post or series of posts giving a basic tensorflow implementation of a simple deep reinforcement learning model.</p>
            
            <a href="https://jmf1sh.github.io/posts/2018-01-14-reinforcement-learning/">Read more…</a>
            
          </li>
          
          <li >
            <h1><a href="https://jmf1sh.github.io/posts/2017-12-19-variance-stabilizing/" title="Variance Stabilizing Transformations">Variance Stabilizing Transformations</a></h1> 
            <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2017-12-19T13:49:09&#43;01:00">Dec 19, 2017</time>
        </li>
        

        

        <li>One minute read</li>
    </ul>
</aside>

            <p>I want to record here a very interesting thing which I recently discovered, variance-stabilizing transformations. The idea is very simple: suppose we have a random variable $x$, which follows a probability distribution which is parametrized solely by its mean $\mu$, with variance $var(x) = g(\mu)$ a known function of the mean.</p>
            
            <a href="https://jmf1sh.github.io/posts/2017-12-19-variance-stabilizing/">Read more…</a>
            
          </li>
          
          <li >
            <h1><a href="https://jmf1sh.github.io/posts/2015-09-15-santalo-formula/" title="Santalo Formula">Santalo Formula</a></h1> 
            <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2015-09-15T00:00:00Z">Sep 15, 2015</time>
        </li>
        

        

        <li>2 minutes read</li>
    </ul>
</aside>

            <p>This post has been migrated from my old blog, the math-physics learning seminar.
Let $M$ be a simple Riemannian manifold with boundary $\partial M$. For $(x,v) \in SM$, let $\tau(x,v)$ denote the exit time of the geodesic starting at $x$ with tangent vector $v$, i.</p>
            
            <a href="https://jmf1sh.github.io/posts/2015-09-15-santalo-formula/">Read more…</a>
            
          </li>
          
          <li >
            <h1><a href="https://jmf1sh.github.io/posts/2015-09-11-index-form/" title="The Index Form">The Index Form</a></h1> 
            <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2015-09-11T00:00:00Z">Sep 11, 2015</time>
        </li>
        

        

        <li>3 minutes read</li>
    </ul>
</aside>

            <p>This post has been migrated from my old blog, the math-physics learning seminar.
Let $f: [0,T] \times (-\epsilon, \epsilon) \to M$ be a family of parametrized curves in a Riemannian manifold $(M, g)$.</p>
            
            <a href="https://jmf1sh.github.io/posts/2015-09-11-index-form/">Read more…</a>
            
          </li>
          
          <li >
            <h1><a href="https://jmf1sh.github.io/posts/2015-09-03-boundary-distance/" title="Boundary Distance">Boundary Distance</a></h1> 
            <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2015-09-03T00:00:00Z">Sep 3, 2015</time>
        </li>
        

        

        <li>3 minutes read</li>
    </ul>
</aside>

            <p>This post has been migrated from my old blog, the math-physics learning seminar.
Recently, I&rsquo;ve been learning some topics related to machine learning, and especially manifold learning. These both fall under the general notion of inverse problems: given some mathematical object $X$ (it could be a function $f: A \to B$, or a Riemannian manifold $(M,g)$, or a probability measure $d\mu$ on a space $X$, etc.</p>
            
            <a href="https://jmf1sh.github.io/posts/2015-09-03-boundary-distance/">Read more…</a>
            
          </li>
          
          <li >
            <h1><a href="https://jmf1sh.github.io/posts/2015-08-31-hamilton-jacobi-distance/" title="Hamilton-Jacobi equation and Riemannian distance">Hamilton-Jacobi equation and Riemannian distance</a></h1> 
            <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2015-08-31T00:00:00Z">Aug 31, 2015</time>
        </li>
        

        

        <li>4 minutes read</li>
    </ul>
</aside>

            <p>This post has been migrated from my old blog, the math-physics learning seminar.
Consider the cotangent bundle $T^\ast X$ as a symplectic manifold with canonical symplectic form $\omega$. Consider the Hamilton-Jacobi equation</p>
            
            <a href="https://jmf1sh.github.io/posts/2015-08-31-hamilton-jacobi-distance/">Read more…</a>
            
          </li>
          
          <li >
            <h1><a href="https://jmf1sh.github.io/posts/2015-08-18-classical-partition-function/" title="Classical partition function">Classical partition function</a></h1> 
            <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2015-08-18T00:00:00Z">Aug 18, 2015</time>
        </li>
        

        

        <li>3 minutes read</li>
    </ul>
</aside>

            <p>This post has been migrated from my old blog, the math-physics learning seminar.
Let $(M, \omega)$ be a symplectic manifold of dimension $2n$, and let $H: M \to \mathbf{R}$ be a classical Hamiltonian.</p>
            
            <a href="https://jmf1sh.github.io/posts/2015-08-18-classical-partition-function/">Read more…</a>
            
          </li>
          
          <li >
            <h1><a href="https://jmf1sh.github.io/posts/2015-01-29-generalized-geometry/" title="What is generalized geometry?">What is generalized geometry?</a></h1> 
            <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2015-01-29T00:00:00Z">Jan 29, 2015</time>
        </li>
        

        

        <li>7 minutes read</li>
    </ul>
</aside>

            <p>This post has been migrated from my old blog, the math-physics learning seminar.
The following are my notes for a short introductory talk. References below are not intended to be comprehensive!</p>
            
            <a href="https://jmf1sh.github.io/posts/2015-01-29-generalized-geometry/">Read more…</a>
            
          </li>
          
        </ul>
      </section>



</main>
    <footer>
        <h6> |
            Rendered by <a href="https://gohugo.io" title="Hugo">Hugo</a> |
            <a href="https://jmf1sh.github.io/index.xml">Subscribe</a></h6>
    </footer>
</div>
<script src="https://jmf1sh.github.io/js/scripts.js"></script>

</body>

</html>
