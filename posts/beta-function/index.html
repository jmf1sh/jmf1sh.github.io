<!doctype html>

<html lang="en-us">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>jonathan&#39;s blog</title>
    <meta name="description" content="The HTML5 Herald">
    <meta name="author" content="Jonathan Fisher">
    <meta name="generator" content="Hugo 0.42.2" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css?family=Roboto+Slab|Ruda" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="https://jmf1sh.github.io/css/styles.css" />

    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body>
    <div id="container">
        <header>
            <h1>
                <a href="https://jmf1sh.github.io/">jonathan&rsquo;s blog</a>
            </h1>

            <ul id="social-media">
                
        
                
                <li><a href="https://www.linkedin.com/in/jonathanfisher10"><i class="fa fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
                
        
                
                <li><a href="https://github.com/jmf1sh"><i class="fa fa-github fa-lg" aria-hidden="true"></i></a></li>
                
        
                
                <li><a href="https://gitlab.com/jmf1sh"><i class="fa fa-gitlab fa-lg" aria-hidden="true"></i></a></li>
                
        
                
        
                
            </ul>
            
            <p><em>mathematics, physics, programming, and machine learning</em></p>
            
        </header>

        
<nav>
    <ul>
        
        <li>
            <a class="" href="https://jmf1sh.github.io/posts/">
                <i class="fa-li fa  fa-lg"></i><span>Posts</span>
            </a>
        </li>
        
        <li>
            <a class="" href="https://jmf1sh.github.io/about/">
                <i class="fa-li fa  fa-lg"></i><span>About</span>
            </a>
        </li>
        
    </ul>
</nav>
        
        <main>




<article>

    <h1>Euler beta function and Dirichlet distribution</h1>

    
        <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2018-07-10T00:00:00Z">Jul 10, 2018</time>
        </li>
        
        

        

        <li>2 min read</li>
    </ul>
</aside>
    

    <p>Let's remind ourselves how about the Euler beta function, defined as</p>

<p><span  class="math">\[ B(x,y) = \int_0^1 t^{x-1} (1-t)^{y-1} dt. \]</span></p>

<p>First, we will express <span  class="math">\(B(x,y)\)</span> in terms of the more familiar Gamma function,
defined as</p>

<p><span  class="math">\[ \Gamma(x) = \int_0^\infty t^{x-1} e^{-t} dt. \]</span></p>

<p>First, we have</p>

<p><span  class="math">\[ \Gamma(x) \Gamma(y) = \int_0^\infty \int_0^\infty s^{x-1} t^{y-1} e^{-s-t} ds dt \]</span></p>

<p>Now, suppose we have a multinomial distribution over <span  class="math">\(k\)</span> classes with probabilites <span  class="math">\(p_1, \ldots, p_k\)</span>. In for $n$ trials, the probability of observing <span  class="math">\(k_i\)</span> instances of class <span  class="math">\(i\)</span> is given by</p>

<p><span  class="math">\[ P(n_1, \ldots, n_k) = {n \choose n_1, \ldots, n_k} \prod_i p_i^{n_i} \]</span></p>

<p>Suppose that initially the probabilities $p_i$ are unknown, and we wish to infer them from
a given observation of $n$ trials. Using a conjugate prior, we can take the unknown probabilites
to follow the Dirichlet distribution</p>

<p><span  class="math">\[ P(p_1, \ldots, p_k) = C_\alpha \prod_i p_i^{\alpha_i-1} \]</span></p>

<p>(where the normalization constant <span  class="math">\(C_\alpha\)</span> can be computed explicitly in terms of the
generalized beta function, or equivalently in terms of the Gamma function).
Using Bayes' theorem we have</p>

<p>
\begin{align*}
P(\alpha | n_1, \ldots, n_k) &= C P(n_1, \ldots, n_k | \alpha) P(\alpha) \\
&= C' \prod_i p_i^{n_i+\alpha_i-1}
\end{align*}
</p>

<p>This gives a new Dirichlet distribution with <span  class="math">\(\alpha_i\)</span> replaced by <span  class="math">\(\alpha_i+n_i\)</span>.
Taking expectation values of the posterior we have</p>

<p><span  class="math">\(E[p_i] = \frac{n_i+\alpha_i}{n+\sum_i \alpha_i}\)</span>.</p>

<p>Taking the Jeffreys prior <span  class="math">\(\alpha_i = 1/2\)</span>, we have finally</p>

<p><span  class="math">\(E[p_i] = \frac{n_i+1/2}{n+n/2}\)</span>.</p>

<p>Note that this assigns a non-zero value to <span  class="math">\(p_i\)</span> even in the case that <span  class="math">\(n_i = 0\)</span>!</p>

<p>For comparison, consider taking instead the estimate from maximum likelihood. We have</p>

<p><span  class="math">\[ \log L(p, n) = C + \sum_i n_i \log p_i. \]</span></p>

<p>Maximizing subject to the constraints <span  class="math">\(\sum_i p_i = 1\)</span>, we have</p>

<p><span  class="math">\[ n_i / p_i = \lambda \]</span></p>

<p>for some Lagrange multiplier <span  class="math">\(\lambda\)</span>, for all <span  class="math">\(i\)</span>. Hence we obtain</p>

<p><span  class="math">\[ p_i = n_i / n \]</span></p>

<p>and note that in this case, we assign zero probability to any class which is not observed.
So we see that the expected distribution derived though Bayesian inference is more
conservative than maximum likelihood, in the sense that zero probabilities are assigned
only in the limit of infinitely many trials.</p>


</article>


<section class="post-nav">
    <ul>
        
        <li>
            <a href="https://jmf1sh.github.io/posts/reinforcement-learning/"><i class="fa fa-chevron-circle-left"></i> Introduction to Reinforcement Learning</a>
        </li>
        
        
    </ul>
</section>
    
        <section class="comments-block">
      <button id="show-comments" style="display: none;"><i class="fa fa-comments-o"></i> Add/View Comments</button>
</section>

<section id="disqus_thread"></section>

<script>
      (function () {
            
            
            if (window.location.hostname == "localhost")
                  return;

            var disqus_loaded = false;
            var disqus_shortname = 'jmf1sh';
            var disqus_button = document.getElementById("show-comments");

            disqus_button.style.display = "";
            disqus_button.addEventListener("click", disqus, false);

            function disqus() {

                  if (!disqus_loaded) {
                        disqus_loaded = true;

                        var e = document.createElement("script");
                        e.type = "text/javascript";
                        e.async = true;
                        e.src = "//" + disqus_shortname + ".disqus.com/embed.js";
                        (document.getElementsByTagName("head")[0] ||
                              document.getElementsByTagName("body")[0])
                        .appendChild(e);

                        
                        document.getElementById("show-comments").style.display = "none";
                  }
            }

            
            var hash = window.location.hash.substr(1);
            if (hash.length > 8) {
                  if (hash.substring(0, 8) == "comment-") {
                        disqus();
                  }
            }

            
            if (/bot|google|baidu|bing|msn|duckduckgo|slurp|yandex/i.test(navigator.userAgent)) {
                  disqus();
            }
      })();
</script>
    





</main>
    <footer>
        <h6> | 
            Rendered by <a href="https://gohugo.io" title="Hugo">Hugo</a> |
            <a href="https://jmf1sh.github.io/index.xml">Subscribe</a></h6>
    </footer>
</div>
<script src="https://jmf1sh.github.io/js/scripts.js"></script>
</body>

</html>