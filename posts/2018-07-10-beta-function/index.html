<!doctype html>

<html lang="en-us">

<head>
  <title>Euler beta function and Dirichlet distribution - jonathan&#39;s blog</title>
  <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="description" content="The HTML5 Herald" />
<meta name="author" content="Jonathan Fisher" /><meta property="og:title" content="Euler beta function and Dirichlet distribution" />
<meta property="og:description" content="Let&rsquo;s remind ourselves how about the Euler beta function, defined as
\[ B(x,y) = \int_0^1 t^{x-1} (1-t)^{y-1} dt. \]
First, we will express \(B(x,y)\) in terms of the more familiar Gamma function, defined as" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jmf1sh.github.io/posts/2018-07-10-beta-function/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-07-10T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2018-07-10T00:00:00&#43;00:00" />


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Euler beta function and Dirichlet distribution"/>
<meta name="twitter:description" content="Let&rsquo;s remind ourselves how about the Euler beta function, defined as
\[ B(x,y) = \int_0^1 t^{x-1} (1-t)^{y-1} dt. \]
First, we will express \(B(x,y)\) in terms of the more familiar Gamma function, defined as"/>

<meta name="generator" content="Hugo 0.81.0" />
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />
  <link rel="stylesheet" href="https://jmf1sh.github.io/fontawesome/css/all.min.css" />
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab|Ruda" />
  
  
  <link rel="stylesheet" type="text/css" href="https://jmf1sh.github.io/css/styles-light.css" />
  </head>

<body>
  <div id="container">
    <header>
      <h1>
                <a href="https://jmf1sh.github.io/">jonathan&rsquo;s blog</a>
            </h1>

      <ul id="social-media">
             <li>
               <a href="https://github.com/jmf1sh" title="GitHub">
               <i class="fab fa-github fa-lg"></i>
               </a>
             </li>
             <li>
               <a href="https://gitlab.com/jmf1sh" title="GitLab">
               <i class="fab fa-gitlab fa-lg"></i>
               </a>
             </li>
             <li>
               <a href="https://linkedin.com/in/jonathanfisher10" title="LinkedIn">
               <i class="fab fa-linkedin fa-lg"></i>
               </a>
             </li>
      </ul>
      
      <p><em>mathematics, physics, programming, and machine learning</em></p>
      
    </header>

    
<nav>
    <ul>
        
        <li>
            <a class="" href="https://jmf1sh.github.io/categories">
                <i class="fa-li fa  fa-lg"></i><span>Categories</span>
            </a>
        </li>
        
        <li>
            <a class="" href="https://jmf1sh.github.io/tags">
                <i class="fa-li fa  fa-lg"></i><span>Tags</span>
            </a>
        </li>
        
        <li>
            <a class="" href="https://jmf1sh.github.io/posts/">
                <i class="fa-li fa  fa-lg"></i><span>Posts</span>
            </a>
        </li>
        
        <li>
            <a class="" href="https://jmf1sh.github.io/about/">
                <i class="fa-li fa  fa-lg"></i><span>About</span>
            </a>
        </li>
        
    </ul>
</nav>


    <main>




<article>

    <h1>Euler beta function and Dirichlet distribution</h1>

    
      <aside>
    <ul>
        <li>
            <time class="post-date" datetime="2018-07-10T00:00:00Z">Jul 10, 2018</time>
        </li>
        
        

        

        <li>2 minute read</li>
    </ul>
</aside>

    

    


    <p>Let&rsquo;s remind ourselves how about the Euler beta function, defined as</p>
<p>\[ B(x,y) = \int_0^1 t^{x-1} (1-t)^{y-1} dt. \]</p>
<p>First, we will express \(B(x,y)\) in terms of the more familiar Gamma function,
defined as</p>
<p>\[ \Gamma(x) = \int_0^\infty t^{x-1} e^{-t} dt. \]</p>
<p>First, we have</p>
<p>\[ \Gamma(x) \Gamma(y) = \int_0^\infty \int_0^\infty s^{x-1} t^{y-1} e^{-s-t} ds dt \]</p>
<p>Now, suppose we have a multinomial distribution over \(k\) classes with probabilites \(p_1, \ldots, p_k\). In for \(n\) trials, the probability of observing \(k_i\) instances of class \(i\) is given by</p>
<p>\[ P(n_1, \ldots, n_k) = {n \choose n_1, \ldots, n_k} \prod_i p_i^{n_i} \]</p>
<p>Suppose that initially the probabilities \(p_i\) are unknown, and we wish to infer them from
a given observation of \(n\) trials. Using a conjugate prior, we can take the unknown probabilites
to follow the Dirichlet distribution</p>
<p>\[ P(p_1, \ldots, p_k) = C_\alpha \prod_i p_i^{\alpha_i-1} \]</p>
<p>(where the normalization constant \(C_\alpha\) can be computed explicitly in terms of the
generalized beta function, or equivalently in terms of the Gamma function).
Using Bayes' theorem we have</p>
<p>\[
\begin{aligned}
P(\alpha | n_1, \ldots, n_k) &amp;= C P(n_1, \ldots, n_k | \alpha) P(\alpha) \cr
&amp;= C' \prod_i p_i^{n_i+\alpha_i-1}
\end{aligned}
\]</p>
<p>This gives a new Dirichlet distribution with \(\alpha_i\) replaced by \(\alpha_i+n_i\).
Taking expectation values of the posterior we have</p>
<p>\[E[p_i] = \frac{n_i+\alpha_i}{n+\sum_i \alpha_i}\].</p>
<p>Taking the Jeffreys prior \(\alpha_i = 1/2\), we have finally</p>
<p>\[E[p_i] = \frac{n_i+1/2}{n+n/2}\].</p>
<p>Note that this assigns a non-zero value to \(p_i\) even in the case that \(n_i = 0\)!</p>
<p>For comparison, consider taking instead the estimate from maximum likelihood. We have</p>
<p>\[ \log L(p, n) = C + \sum_i n_i \log p_i. \]</p>
<p>Maximizing subject to the constraints \(\sum_i p_i = 1\), we have</p>
<p>\[ n_i / p_i = \lambda \]</p>
<p>for some Lagrange multiplier \(\lambda\), for all \(i\). Hence we obtain</p>
<p>\[ p_i = n_i / n \]</p>
<p>and note that in this case, we assign zero probability to any class which is not observed.
So we see that the expected distribution derived though Bayesian inference is more
conservative than maximum likelihood, in the sense that zero probabilities are assigned
only in the limit of infinitely many trials.</p>


</article>


<section class="post-nav">
    <ul>
        
        <li>
            <a href="https://jmf1sh.github.io/posts/2018-01-14-reinforcement-learning/"><i class="fa fa-chevron-circle-left"></i> Introduction to Reinforcement Learning</a>
        </li>
        
        
        <li>
            <a href="https://jmf1sh.github.io/posts/2018-12-24-vae/">Variational autoencoders <i class="fa fa-chevron-circle-right"></i> </a>
        </li>
        
    </ul>
</section>
  
    
    
        <section class="comments-block">
      <button id="show-comments" style="display: none;"><i class="fa fa-comments"></i> Add/View Comments</button>
</section>

<section id="disqus_thread"></section>

<script>
      (function () {
            
            
            if (window.location.hostname == "localhost")
                  return;

            var disqus_loaded = false;
            var disqus_shortname = 'jmf1sh';
            var disqus_button = document.getElementById("show-comments");

            var disqus_autoload =  null ;
            var disable_comment =  null ;

            if (disable_comment)
                  return;

            disqus_button.style.display = "";

            if (disqus_autoload){
                  disqus();
            }else{
                  disqus_button.addEventListener("click", disqus, false);
            }

            function disqus() {

                  if (!disqus_loaded) {
                        disqus_loaded = true;

                        var e = document.createElement("script");
                        e.type = "text/javascript";
                        e.async = true;
                        e.src = "//" + disqus_shortname + ".disqus.com/embed.js";
                        (document.getElementsByTagName("head")[0] ||
                              document.getElementsByTagName("body")[0])
                        .appendChild(e);

                        
                        document.getElementById("show-comments").style.display = "none";
                  }
            }

            
            var hash = window.location.hash.substr(1);
            if (hash.length > 8) {
                  if (hash.substring(0, 8) == "comment-") {
                        disqus();
                  }
            }

            
            if (/bot|google|baidu|bing|msn|duckduckgo|slurp|yandex/i.test(navigator.userAgent)) {
                  disqus();
            }
      })();
</script>

    
  





</main>
    <footer>
        <h6> |
            Rendered by <a href="https://gohugo.io" title="Hugo">Hugo</a> |
            <a href="https://jmf1sh.github.io/index.xml">Subscribe </a></h6>
    </footer>
</div>
<script src="https://jmf1sh.github.io/js/scripts.js"></script>


</body>

</html>

