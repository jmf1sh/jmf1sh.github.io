To do
-------------------------------------------------------------------------------
* [Maximum Likelihood Estimation of Observer Error‚ÄêRates Using the EM Algorithm](https://www.semanticscholar.org/paper/Maximum-Likelihood-Estimation-of-Observer-Using-the-Dawid-Skene/c80c7ab615b2fad5148a7848dbdd26a2dc50dd3d)
* [Learning From Noisy Labels By Regularized Estimation Of Annotator Confusion](https://arxiv.org/abs/1902.03680)
* Notes for DeepMind's RL course
* Hoeffding's inequality for bounds when val=test [link](https://people.orie.cornell.edu/mru8/orie4741/lectures/generalization.pdf)
* Comparison of Hamilton-Jacobi equation with Bellman equation
* Toy autograd engine
* Epistemic and aleatoric uncertainty
* Active learning, BALD
* [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
* Neural tangent kernel
* Gaussian processes
* Relation of GPs to quantum field theory
* [Hopfield networks](https://arxiv.org/abs/2008.02217)
* Self attention & transformers
* Restricted Boltzmann machine and Ising model
* Information bottleneck
* Basics of information theory
* Kolmogorov complexity
* Manifold learning
* Information geometry (https://arxiv.org/abs/1808.08271)
* Gromov-Hausdorff distance
* Complex analysis eye candy
* Imitation learning
* Polygon spaces
* Playing with Spacenet in QGIS
* Q-learning for Atari
* Something with NLP
* Visual transformers
* Play with rlstructures https://github.com/facebookresearch/rlstructures
* Instance segmentation
* NAND to Tetris
* Microarchitecture design
* CUDA programming
* Pytorch for scientific computing
* Bayesian NNs
* Hamiltonian Monte-Carlo
* Stochastic weight averaging
* Categorical machine learning
* No free lunch theorem
* Jeffreys prior
* Conjugate prior
* Flat minima and natural gradients
* Relation between natural gradients and neural tangent kernel
* Linear approximation of DNN (e.g. https://arxiv.org/abs/2103.01439)
* Fast transformers
* RNNs, LSTM, GRU
* Relation between transformers and RNNs (https://arxiv.org/abs/2006.16236)
* Representation learning, SimCLR, MoCo, BYOL, etc.
* Efficient market hypothesis
* Financial derivatives
* Portfolio management theory
* Mathematical finance
* Random walks
* Chaos
* Perturbation theory in quantum mechanics
* Schwinger-Dyson equation
* Born approximation, DWBA
* Theory of crystal diffraction
* geometric optics
* Rendering equation, ray tracing
* Marching cubes
* Neural radiance fields
* Some demos with shadertoy
* Minimal lisp implementation
* Black hole thermodynamics and Bekenstein bound
* Causal sets
* Loop quantum gravity (or something related)
* C*-algebras and QM (https://arxiv.org/abs/2104.02038)
* Optimal transport
* Control theory
* Non-commutative geometry, spectral triples
* Resurgence in QM/QFT
* Decoherence
* Bethe Ansatz
* Quantum information theory
* It from Bit program
* Lambda calculus and Y combinator
* Hawking radiation
* Conformal field theory
* Lax pairs, integrability
* Nahm's equations & Lax pair formulation with spectral curve (https://en.wikipedia.org/wiki/Nahm_equations, https://projecteuclid.org/journals/communications-in-mathematical-physics/volume-89/issue-2/On-the-construction-of-monopoles/cmp/1103922679.full)
* KdV & integrability
* Pentagram map
* Gromov non-squeezing theorem as analogue of Heisenberg uncertainty relation
* Information closure theory of consciousness (https://www.frontiersin.org/articles/10.3389/fpsyg.2020.01504/full)
